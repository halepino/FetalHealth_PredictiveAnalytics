{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933db124",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier on Fetal Heart Health Predictors  \n",
    "\n",
    "***Karlie Schwartzwald  \n",
    "DSC 630 Winter 2022  \n",
    "Bellevue University***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10f5e7",
   "metadata": {},
   "source": [
    "**Change Control Log:**  \n",
    "\n",
    "Change#: 1  \n",
    "Change(s) Made:  Imported and cleaned data. Trained Naieve Bayes and ran accuracy scores.    \n",
    "Date of Change:  1/28/2023  \n",
    "Author: Karlie Schwartzwald  \n",
    "Change Approved by: Karlie Schwartzwald  \n",
    "Date Moved to Production:   \n",
    "\n",
    "Change#: 2  \n",
    "Change(s) Made:  Tried Complement NB and got better accuracy score. Then did hyperparameter tuning.     \n",
    "Date of Change:  1/29/2023  \n",
    "Author: Karlie Schwartzwald  \n",
    "Change Approved by: Karlie Schwartzwald  \n",
    "Date Moved to Production:   \n",
    "\n",
    "Change#: 3  \n",
    "Change(s) Made:  Removed anythng not directly relevent to complement NB     \n",
    "Date of Change:  2/10/2023  \n",
    "Author: Karlie Schwartzwald  \n",
    "Change Approved by: Karlie Schwartzwald  \n",
    "Date Moved to Production:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e437f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce918434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fnr_fpr_tnr(cm):\n",
    "    \"\"\"\n",
    "    This function returns class-wise TPR, FNR, FPR & TNR\n",
    "    [[cm]]: a 2-D array of a multiclass confusion matrix\n",
    "            where horizontal axes represent actual classes\n",
    "            and vertical axes represent predicted classes\n",
    "    {output}: a dictionary of class-wise accuracy parameters\n",
    "    \"\"\"\n",
    "    dict_metric = dict()\n",
    "    n = len(cm[0])\n",
    "    row_sums = cm.sum(axis=1)\n",
    "    col_sums = cm.sum(axis=0)\n",
    "    array_sum = sum(sum(cm))\n",
    "    #initialize a blank nested dictionary\n",
    "    for i in range(1, n+1):\n",
    "        keys = str(i)\n",
    "        dict_metric[keys] = {\"TPR\":0, \"FNR\":0, \"FPR\":0, \"TNR\":0}\n",
    "    # calculate and store class-wise TPR, FNR, FPR, TNR\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                keys = str(i+1)\n",
    "                tp = cm[i, j]\n",
    "                fn = row_sums[i] - cm[i, j]\n",
    "                dict_metric[keys][\"TPR\"] = tp / (tp + fn)\n",
    "                dict_metric[keys][\"FNR\"] = fn / (tp + fn)\n",
    "                fp = col_sums[i] - cm[i, j]\n",
    "                tn = array_sum - tp - fn - fp\n",
    "                dict_metric[keys][\"FPR\"] = fp / (fp + tn)\n",
    "                dict_metric[keys][\"TNR\"] = tn / (fp + tn)\n",
    "    return dict_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0601fe1",
   "metadata": {},
   "source": [
    "## Null Model for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39959648",
   "metadata": {},
   "source": [
    "Below we will calculate the accuracy of a model that simply always chooses the most likely outcome. In this case the model will predict that all fetal heart health outcomes are healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abfb4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fetal_health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4d4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate target feature\n",
    "x = df.drop(['fetal_health'], axis=1)\n",
    "y = df['fetal_health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b488d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b338b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    326\n",
       "2.0     58\n",
       "3.0     42\n",
       "Name: fetal_health, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution in test set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae79f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy score: 0.7790\n"
     ]
    }
   ],
   "source": [
    "# check null accuracy score\n",
    "null_accuracy = (497/(497+94+47))\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ac1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we need to score above 78% to be a good model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0e36f",
   "metadata": {},
   "source": [
    "## Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3b2db",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e369794",
   "metadata": {},
   "source": [
    "For the Naïve Bayes classifier, we chose to use the Complement Naïve Bayes due to its ability to handle imbalanced classes in the target feature. We used a train-test-split ratio of 80/20 because it yielded the highest accuracy of any ratio we tested, without overfitting the model too much. We then applied a minmax scalar because the model only accepts positive values as input. After that we did a grid search to perform hyperparameter tuning on the only hyperparameter in a complement naïve Bayes model, using accuracy as our scoring metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ca67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated variables\n",
    "# 0.9 Threshold as described in Milestone 3\n",
    "\n",
    "# Histogram mode and median highly correlate with hisogram mean\n",
    "df.drop(['histogram_median', 'histogram_mode', ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088a922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate target feature\n",
    "X = df.drop(['fetal_health'], axis=1)\n",
    "y = df['fetal_health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e3d8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46775319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "# I had to use this particular scalar because of negative numbers error\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c1145",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870d3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch values\n",
    "grid_vals = {\"alpha\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, \n",
    "                       1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
    "                      10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
    "                      20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
    "                      30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
    "                      40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
    "                      50, 51, 52, 53, 54, 55, 56, 57, 48, 59,\n",
    "                      60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
    "                      70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
    "                      80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
    "                      90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef181c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch model\n",
    "grid_lr = GridSearchCV(estimator=ComplementNB(), param_grid=grid_vals, scoring='accuracy', \n",
    "                       cv=6, refit=True, return_train_score=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f7edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Prediction\n",
    "tuned_classifier = grid_lr.fit(X_train, y_train)\n",
    "tuned_preds = grid_lr.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c8c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Gaussian NB Parameter: {'alpha': 49}\n"
     ]
    }
   ],
   "source": [
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Gaussian NB Parameter: {}\".format(grid_lr.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa17711",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fb27b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate\n",
    "scores3 = cross_validate(tuned_classifier, X_train, y_train, return_train_score=True)\n",
    "scores_df3 = pd.DataFrame(scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7af2324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       1.152596\n",
       "score_time     0.000000\n",
       "test_score     0.847059\n",
       "train_score    0.850147\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df3.mean()\n",
    "# Looks like we could be over fitting but the model is performing better than null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3ca5d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPR</th>\n",
       "      <th>FNR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.892638</td>\n",
       "      <td>0.107362</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.105978</td>\n",
       "      <td>0.894022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.973958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TPR       FNR       FPR       TNR\n",
       "1  0.892638  0.107362  0.300000  0.700000\n",
       "2  0.534483  0.465517  0.105978  0.894022\n",
       "3  0.595238  0.404762  0.026042  0.973958"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, tuned_preds)\n",
    "conf_mat = pd.DataFrame(get_tpr_fnr_fpr_tnr(cm)).transpose()\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6414de9",
   "metadata": {},
   "source": [
    "In evaluation of the complement naïve Bayes model, we primarily used accuracy but also looked more closely at false positive rates, false negative rates, true positive rates, and true negative rates. For the test data, the average accuracy was approximately 84.7% and for the test data the average accuracy was approximately 85%. This could imply some overfitting is happening with this model. But when we look more closely at the true positive rates for each of the categorical outcomes, we do have some serious concerns. It appears that this model is very accurate at predicting healthy outcomes, with an accuracy of 89%, but much worse at correctly predicting other fetal health outcomes. Since that is really the purpose of this model, we conclude that this model is not suitable for deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86a2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
